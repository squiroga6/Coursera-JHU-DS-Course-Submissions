---
title: "Practical Machine Learning Final Project"
author: "Samuel Quiroga"
date: "October 17, 2018"
output: html_document
---

```{r setup, include=TRUE,results='hide',message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(parallel)
library(doParallel)
library(AppliedPredictiveModeling)
```

## Practical Machine Learning Peer-graded Assignment

### Exectuive Summary
The goal of this assignment is to predict whether the manner in which an exercise was performed correctly or incorrectly. The data was collected from a study where six participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in 5 different ways (classes A through E). For more information on this study, please go to [http://groupware.les.inf.puc-rio.br/har].

Here are some important findings:

* An accuracy above 99% was attained using a random forest algorithm with 10-fold cross-validations repeated 3 times.
* 62.5% of the variables of the dataset contained more than 97.9% of `NULL` values which were removed for modelling
* 54 variables were used to predict the quality variable `classe`

### Getting and cleaning data
Download datasets from links provided.
```{r}
url.training <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url.testing <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
# check if directory exists
if(!file.exists("data")){dir.create("data")}
# download data to working directory ("data" folder)
download.file(url.training,"./data/training.csv")
download.file(url.testing,"./data/testing.csv")
```
Load the training and testing datasets into the R environment.We also assign `NA` values here.
```{r}
training.raw <- read.csv("./data/training.csv",na.strings = c("","#DIV/0!","NA"))
test.raw <- read.csv("./data/testing.csv",na.strings = c("","#DIV/0!","NA"))
```
Taking a look at the dimensions of the training data, we can see that it has 19622 observations and 160 variables.
```{r}
dim(training.raw)
```
We should first understand how many null values are in each colum so that in case there are columns with large amount of `NULL` values, for example more than 95%, then we can safetly remove those variables from the prediction model.
```{r}
null.cols <- as.data.frame(apply(training.raw,2,function(x){sum(is.na(x))}))
colnames(null.cols) <- "Total.Num.Null"; null.cols <- null.cols %>% 
        rownames_to_column("Variable") %>% 
        filter(Total.Num.Null > 0) # filter out columns that do not contain any NULL value
head(null.cols,10)
```
The table above shows 10 out of 100 columns that contain more than one `NULL` value.Below is a summary of columns with null values:
```{r}
summary(null.cols)
```
Therefore, we see that out of 160 variables in the dataset, 100 (62.5%) of them have at least 19216 null values, which is 97.9% of all the observations. Therefore we will remove these 100 columns from the training and test data sets for modeling purposes. 

Now, we are taking the deliberate approach of removing some variables. We will remove the `X` (row names), `raw_timestamp_part_1`, `raw_timestamp_part_2`, `cvtd_timestamp` (all time series), `new_window` and `num_window` columns as it is assumed that this will not be used for predicting the class variable. The assumption is based on the fact that the time will not affect the type `classe`. 
```{r}
cols.remove <- null.cols$Variable
training.main <- training.raw %>% select(
                                         -X,
                                         -raw_timestamp_part_1,-raw_timestamp_part_2,
                                         -cvtd_timestamp,-new_window,-num_window,
                                         -cols.remove)
test.main <- test.raw %>% select(
                                 -X,
                                 -raw_timestamp_part_1,-raw_timestamp_part_2,
                                 -cvtd_timestamp,-new_window,-num_window,
                                 -cols.remove)
dim(training.main)
```
We see that our training data set is now 54 columns, including the variable we want to predict `classe` (down from 160). This datset will be used to model the `classe` variable.

### Data Visualization
Since we are still looking at a large number of observations (19622) and a large number of variables (52), for the purposes of efficiency we'll sample the first 100 observations from each `classe` type and look at the columns that contain data for `total_accel`, total acceleration for each movement. Here we will build a scatter matrix plot.
```{r, fig.align='center',fig.height=9}
# resample data
training.sample <- training.main %>%
        select(matches("total_accel|classe")) %>%
        group_by(classe) %>%
        sample_n(100)
# create plot
transparentTheme(trans = 0.4)
featurePlot(x=training.sample[,names(training.sample)!="classe"],
            y = training.sample$classe,
            plot = "pairs",
            ## add key at the top
            auto.key = list(columns=3))
```
We can observe that for some of the plots, there are clear variations in the data while others contain clusters that can be split.

### Pre-Processing
Check for near-zero variance predictor and identify and remove if they exist prior to modelling.
```{r}
nzv <- nearZeroVar(training.main)
filt.temp <- training.main[,-nzv]
dim(filt.temp)
```
Therefore we can see that there are not any near-zero variance predictors in the dataset.

We should also identify highly correlated predictors (>99.9% correlation) so that we can remove them if necessary
```{r}
dataCorr <- cor(training.main[,-c(1,54)]) # remove variable to predict and user name
highCorr <- sum(abs(dataCorr[upper.tri(dataCorr)]) > .999)
highCorr
```
We can see that there no predictors that are highly correlated.

### Develop Model
Since the data size is large enough (>19000 samples) and this is a multi-class classification setting (we are trying to predict different classes in `classe`), we will use repeated k-fold cross validation for this data set. For the prediction method, we wil use Random Forests. First we will set up parallel processing to increase computing speeds.

Set up parallel processing with `parallel` and `doParallel` packages.
```{r}
cluster <- makeCluster(detectCores()-1)
registerDoParallel(cluster)
```
Since we will do 3 repeats of 10-Fold CV, will need  to set a seed for each re-sample to ensure repeatability.
```{r}
set.seed(654)
my.seeds <- vector(mode="list",length = 31) # generate list of seeds, one for each sample plus 1 (3 repeats * 10 folds + 1)
for(i in 1:30) my.seeds[[i]] <- sample.int(1000,22)
# for the last model
my.seeds[[31]] <- sample.int(1000,1)
```
Configure the trainControl function with repeated cross-validation, re-sampled ten times and repeated 3 times.
```{r}
train.control <- trainControl(method = "repeatedcv",number=10,repeats = 3,allowParallel = T, seeds = my.seeds)
```
Develope training model using the `trainContro()` object that we just created.
```{r}
model <- train(classe ~ .,data=training.main,method="rf",trControl=train.control)
```
De-register parallel processing cluster to force R to return to single threaded processing.
```{r}
stopCluster(cluster)
registerDoSEQ()
```


### Assess Model Accuracy
First see the model results :
```{r}
model
```
Fold accuracy :  we can also observe the accuracy for each fold.
```{r}
model$resample
```
See the confustion matrix
```{r}
confusionMatrix(model)
```
Therefore we can see that we have attained a model accuracy of 99.5% using repeated k-fold cross validation.


